{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flights: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 27 more fields]\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flights = (spark.read.format(\"com.databricks.spark.csv\")\n",
    "               .option(\"header\", \"true\")\n",
    "               .option(\"inferSchema\", \"true\")\n",
    "               .option(\"delimiter\", \",\")\n",
    "               .load(\"/spark-files/2008.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- DayofMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- DepTime: string (nullable = true)\n",
      " |-- CRSDepTime: integer (nullable = true)\n",
      " |-- ArrTime: string (nullable = true)\n",
      " |-- CRSArrTime: integer (nullable = true)\n",
      " |-- UniqueCarrier: string (nullable = true)\n",
      " |-- FlightNum: integer (nullable = true)\n",
      " |-- TailNum: string (nullable = true)\n",
      " |-- ActualElapsedTime: string (nullable = true)\n",
      " |-- CRSElapsedTime: string (nullable = true)\n",
      " |-- AirTime: string (nullable = true)\n",
      " |-- ArrDelay: string (nullable = true)\n",
      " |-- DepDelay: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- Dest: string (nullable = true)\n",
      " |-- Distance: integer (nullable = true)\n",
      " |-- TaxiIn: string (nullable = true)\n",
      " |-- TaxiOut: string (nullable = true)\n",
      " |-- Cancelled: integer (nullable = true)\n",
      " |-- CancellationCode: string (nullable = true)\n",
      " |-- Diverted: integer (nullable = true)\n",
      " |-- CarrierDelay: string (nullable = true)\n",
      " |-- WeatherDelay: string (nullable = true)\n",
      " |-- NASDelay: string (nullable = true)\n",
      " |-- SecurityDelay: string (nullable = true)\n",
      " |-- LateAircraftDelay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res14: org.apache.spark.sql.DataFrame = [UniqueCarrier: string, FlightNum: int ... 2 more fields]\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.select(\"UniqueCarrier\", \"FlightNum\", \"DepDelay\", \"ArrDelay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "delayedFlights: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [UniqueCarrier: string, DepDelay: string]\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val delayedFlights = flights.select(\"UniqueCarrier\", \"DepDelay\").filter($\"DepDelay\" > 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+\n",
      "|UniqueCarrier|DepDelay|\n",
      "+-------------+--------+\n",
      "|           WN|      34|\n",
      "|           WN|      67|\n",
      "|           WN|      94|\n",
      "|           WN|      51|\n",
      "|           WN|      32|\n",
      "|           WN|      87|\n",
      "|           WN|      82|\n",
      "|           WN|      39|\n",
      "|           WN|      82|\n",
      "|           WN|      56|\n",
      "|           WN|     315|\n",
      "|           WN|      45|\n",
      "|           WN|      53|\n",
      "|           WN|      38|\n",
      "|           WN|      44|\n",
      "|           WN|      58|\n",
      "|           WN|      53|\n",
      "|           WN|      83|\n",
      "|           WN|      57|\n",
      "|           WN|      97|\n",
      "+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "delayedFlights.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numTotalFlights: Long = 7009728\n",
       "numDelayedFlights: Long = 814451\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numTotalFlights = flights.count()\n",
    "val numDelayedFlights = delayedFlights.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "percentageOfDelayedFlights: Float = 11.618868\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val percentageOfDelayedFlights = (numDelayedFlights.toFloat/numTotalFlights*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of delayed flights is: 11.618868%\n"
     ]
    }
   ],
   "source": [
    "println(\"The percentage of delayed flights is: \" + percentageOfDelayedFlights + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's write a function to mark delayed flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions.udf\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions.udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "isDelayedUDF: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,IntegerType,Some(List(StringType)))\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val isDelayedUDF = udf((time: String) => if (time == \"NA\") 0\n",
    "                       else if (time.toInt > 30) 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "flightsWithDelays: org.apache.spark.sql.DataFrame = [Year: int, Month: int ... 5 more fields]\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val flightsWithDelays = flights.select($\"Year\", $\"Month\", $\"DayOfMonth\",\n",
    "                                       $\"UniqueCarrier\", $\"FlightNum\", $\"DepDelay\",\n",
    "                                      isDelayedUDF($\"DepDelay\").alias(\"IsDelayed\")\n",
    "                                      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+-------------+---------+--------+---------+\n",
      "|Year|Month|DayOfMonth|UniqueCarrier|FlightNum|DepDelay|IsDelayed|\n",
      "+----+-----+----------+-------------+---------+--------+---------+\n",
      "|2008|    1|         3|           WN|      335|       8|        0|\n",
      "|2008|    1|         3|           WN|     3231|      19|        0|\n",
      "|2008|    1|         3|           WN|      448|       8|        0|\n",
      "|2008|    1|         3|           WN|     1746|      -4|        0|\n",
      "|2008|    1|         3|           WN|     3920|      34|        1|\n",
      "|2008|    1|         3|           WN|      378|      25|        0|\n",
      "|2008|    1|         3|           WN|      509|      67|        1|\n",
      "|2008|    1|         3|           WN|      535|      -1|        0|\n",
      "|2008|    1|         3|           WN|       11|       2|        0|\n",
      "|2008|    1|         3|           WN|      810|       0|        0|\n",
      "+----+-----+----------+-------------+---------+--------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightsWithDelays.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's find the average of flights that taxi-in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+------------------+\n",
      "|Origin|Dest|         AvgTaxiIn|\n",
      "+------+----+------------------+\n",
      "|   BNA| FSD|              48.0|\n",
      "|   RIC| RDU|              29.0|\n",
      "|   VPS| LGA|              29.0|\n",
      "|   XNA| SGF|              21.0|\n",
      "|   CHA| DTW|20.732394366197184|\n",
      "|   LAX| JAX|              20.5|\n",
      "|   HSV| LGA|20.328358208955223|\n",
      "|   BUR| RNO|              20.0|\n",
      "|   MFE| ATL|              20.0|\n",
      "|   MYR| PHL|              19.0|\n",
      "+------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flights.select(\"Origin\", \"Dest\", \"TaxiIn\")\n",
    "               .groupBy(\"Origin\", \"Dest\")\n",
    "               .agg(avg(\"TaxiIn\")\n",
    "               .alias(\"AvgTaxiIn\"))\n",
    "               .orderBy(desc(\"AvgTaxiIn\"))\n",
    "               .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's run HIVE commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.{SparkContext, SparkConf}\n",
       "import org.apache.spark.sql.hive.HiveContext\n",
       "import org.apache.spark.sql.SQLContext\n",
       "import org.apache.spark.sql.SaveMode\n"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.{SparkContext,SparkConf}\n",
    "import org.apache.spark.sql.hive.HiveContext\n",
    "import org.apache.spark.sql.SQLContext\n",
    "import org.apache.spark.sql.SaveMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "conf: org.apache.spark.SparkConf = org.apache.spark.SparkConf@1de0ea41\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf().setMaster(\"local\").setAppName(\"HiveContext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@793d4617\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = new SparkContext(conf);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hiveContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@119f7694\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val hiveContext:SQLContext = new HiveContext(sc)                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hiveContext.setConf(\"hive.metastore.uiris\",\"thrift://localhost:9083\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default| employee|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hiveContext.tables(\"default\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load a parquet file and write back in another format - then verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "babyNamesDF: org.apache.spark.sql.DataFrame = [year: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val babyNamesDF = hiveContext.read.parquet(\"/spark-files/baby_names.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "babyNamesDF.write.format(\"orc\").mode(SaveMode.Append).saveAsTable(\"babynames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-----------+\n",
      "|database|tableName|isTemporary|\n",
      "+--------+---------+-----------+\n",
      "| default|babynames|      false|\n",
      "| default| employee|      false|\n",
      "+--------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hiveContext.tables(\"default\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>(base) <font color=\"#4E9A06\"><b>hadoopuser@hadoopuser-VirtualBox</b></font>:<font color=\"#3465A4\"><b>~</b></font>$ hdfs dfs -ls /user/hive/warehouse/\n",
    "Found 1 items\n",
    "drwxr-xr-x   - hadoopuser supergroup          0 2024-06-17 12:53 /user/hive/warehouse/babynames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "babyNamesHIVE: org.apache.spark.sql.DataFrame = [year: int, name: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val babyNamesHIVE = hiveContext.table(\"babynames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "babyNamesHIVE.registerTempTable(\"namesforbaby\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+---+\n",
      "|year|   name|    prob|sex|\n",
      "+----+-------+--------+---+\n",
      "|1880|   John|0.081541|boy|\n",
      "|1880|William|0.080511|boy|\n",
      "|1880|  James|0.050057|boy|\n",
      "|1880|Charles|0.045167|boy|\n",
      "|1880| George|0.043292|boy|\n",
      "|1880|  Frank| 0.02738|boy|\n",
      "|1880| Joseph|0.022229|boy|\n",
      "|1880| Thomas|0.021401|boy|\n",
      "|1880|  Henry|0.020641|boy|\n",
      "|1880| Robert|0.020404|boy|\n",
      "|1880| Edward|0.019965|boy|\n",
      "|1880|  Harry|0.018175|boy|\n",
      "|1880| Walter|0.014822|boy|\n",
      "|1880| Arthur|0.013504|boy|\n",
      "|1880|   Fred|0.013251|boy|\n",
      "|1880| Albert|0.012609|boy|\n",
      "|1880| Samuel|0.008648|boy|\n",
      "|1880|  David|0.007339|boy|\n",
      "|1880|  Louis|0.006993|boy|\n",
      "|1880|    Joe|0.006174|boy|\n",
      "+----+-------+--------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hiveContext.sql(\"SELECT * FROM namesforbaby\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use another parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sampleDF: org.apache.spark.sql.DataFrame = [firstName: string, gender: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sampleDF = hiveContext.read.parquet(\"/spark-files/sample.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+----+\n",
      "|firstName|gender|total|year|\n",
      "+---------+------+-----+----+\n",
      "|     Mary|     F| 7065|1880|\n",
      "|     Anna|     F| 2604|1880|\n",
      "|     Emma|     F| 2003|1880|\n",
      "|Elizabeth|     F| 1939|1880|\n",
      "|   Minnie|     F| 1746|1880|\n",
      "| Margaret|     F| 1578|1880|\n",
      "|      Ida|     F| 1472|1880|\n",
      "|    Alice|     F| 1414|1880|\n",
      "|   Bertha|     F| 1320|1880|\n",
      "|    Sarah|     F| 1288|1880|\n",
      "|    Annie|     F| 1258|1880|\n",
      "|    Clara|     F| 1226|1880|\n",
      "|     Ella|     F| 1156|1880|\n",
      "| Florence|     F| 1063|1880|\n",
      "|     Cora|     F| 1045|1880|\n",
      "|   Martha|     F| 1040|1880|\n",
      "|    Laura|     F| 1012|1880|\n",
      "|   Nellie|     F|  995|1880|\n",
      "|    Grace|     F|  982|1880|\n",
      "|   Carrie|     F|  949|1880|\n",
      "+---------+------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleDF.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res61: sampleDF.type = [firstName: string, gender: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampleDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstName: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- total: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sampleDF.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "firstName: org.apache.spark.sql.DataFrame = [firstname: string, year: int]\n"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val firstName = sampleDF.select(\"firstname\", \"year\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+\n",
      "|firstname|year|\n",
      "+---------+----+\n",
      "|     Mary|1880|\n",
      "|     Anna|1880|\n",
      "|     Emma|1880|\n",
      "|Elizabeth|1880|\n",
      "|   Minnie|1880|\n",
      "| Margaret|1880|\n",
      "|      Ida|1880|\n",
      "|    Alice|1880|\n",
      "|   Bertha|1880|\n",
      "|    Sarah|1880|\n",
      "|    Annie|1880|\n",
      "|    Clara|1880|\n",
      "|     Ella|1880|\n",
      "| Florence|1880|\n",
      "|     Cora|1880|\n",
      "|   Martha|1880|\n",
      "|    Laura|1880|\n",
      "|   Nellie|1880|\n",
      "|    Grace|1880|\n",
      "|   Carrie|1880|\n",
      "+---------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "firstName.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res64: Long = 922322\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstName.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res65: Long = 45019\n"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "firstName.select(\"firstname\").distinct.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  firstName|\n",
      "+-----------+\n",
      "|    Michael|\n",
      "|   Jennifer|\n",
      "|Christopher|\n",
      "|      Jason|\n",
      "|      David|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// most popular in 1980...\n",
    "sampleDF.filter(sampleDF(\"year\") === 1980)\n",
    "        .orderBy(sampleDF(\"total\").desc, $\"firstName\")\n",
    "        .select(\"firstName\")\n",
    "        .limit(5).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|  firstName|\n",
      "+-----------+\n",
      "|    Michael|\n",
      "|   Jennifer|\n",
      "|Christopher|\n",
      "|      Jason|\n",
      "|      David|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// or we can use $ for column references\n",
    "sampleDF.filter($\"year\" === 1980)\n",
    "         .orderBy($\"total\".desc, $\"firstName\")\n",
    "         .select(\"firstName\")\n",
    "         .limit(5)\n",
    "         .show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lowerCase: org.apache.spark.sql.expressions.UserDefinedFunction = UserDefinedFunction(<function1>,StringType,Some(List(StringType)))\n"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// popular names in 1890!!!\n",
    "// get lower case\n",
    "\n",
    "val lowerCase = hiveContext.udf.register(\"lower\", (s: String) => s.toLowerCase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "names1890: org.apache.spark.sql.DataFrame = [total1890: int, gender1890: string ... 1 more field]\n"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// filter only names in 1890\n",
    "\n",
    "val names1890 = sampleDF.filter($\"year\" === 1890)\n",
    "                               .select($\"total\".as(\"total1890\"),\n",
    "                                      $\"gender\".as(\"gender1890\"),\n",
    "                                      lower($\"firstName\")\n",
    "                                      .as(\"name1890\"))                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "names1880: org.apache.spark.sql.DataFrame = [total1880: int, gender1880: string ... 1 more field]\n"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// let's add popular names in 1880\n",
    "\n",
    "val names1880 = sampleDF.filter($\"year\" === 1880)\n",
    "                               .select($\"total\".as(\"total1880\"),\n",
    "                                      $\"gender\".as(\"gender1880\"),\n",
    "                                      lower($\"firstName\")\n",
    "                                      .as(\"name1880\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "names1800sJoined: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [total1890: int, gender1890: string ... 4 more fields]\n"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// let's join them\n",
    "\n",
    "val names1800sJoined = names1890.join(names1880, ($\"name1890\" === $\"name1880\")\n",
    "                                     && ($\"gender1890\" === $\"gender1880\"))\n",
    "                                        .orderBy($\"total1890\".as(\"name\"),\n",
    "                                                $\"total1880\", $\"total1890\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+---------+----------+--------+\n",
      "|total1890|gender1890|name1890|total1880|gender1880|name1880|\n",
      "+---------+----------+--------+---------+----------+--------+\n",
      "|        5|         M|    rene|        5|         M|    rene|\n",
      "|        5|         F|  mignon|        5|         F|  mignon|\n",
      "|        5|         M|  payton|        5|         M|  payton|\n",
      "|        5|         M|gustavus|        5|         M|gustavus|\n",
      "|        5|         M|  woodie|        5|         M|  woodie|\n",
      "|        5|         M|     ely|        5|         M|     ely|\n",
      "|        5|         M|schuyler|        5|         M|schuyler|\n",
      "|        5|         M|  samual|        5|         M|  samual|\n",
      "|        5|         M| unknown|        5|         M| unknown|\n",
      "|        5|         F|   lella|        5|         F|   lella|\n",
      "|        5|         M| julious|        5|         M| julious|\n",
      "|        5|         M| murdock|        5|         M| murdock|\n",
      "|        5|         M|    nora|        5|         M|    nora|\n",
      "|        5|         F|roseanna|        5|         F|roseanna|\n",
      "|        5|         M| hershel|        5|         M| hershel|\n",
      "|        5|         F|  elmire|        5|         F|  elmire|\n",
      "|        5|         M|  tilmon|        5|         M|  tilmon|\n",
      "|        5|         F|penelope|        5|         F|penelope|\n",
      "|        5|         M|  myrtle|        5|         M|  myrtle|\n",
      "|        5|         F|   edgar|        5|         F|   edgar|\n",
      "+---------+----------+--------+---------+----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "// let's see what we get\n",
    "names1800sJoined.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
