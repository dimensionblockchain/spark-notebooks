{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe from RDD - SparkSQL is for strutured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[0] at parallelize at <console>:25\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val a = sc.parallelize(1 to 10) // create a simple RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b: org.apache.spark.rdd.RDD[(Int, Int)] = MapPartitionsRDD[1] at map at <console>:26\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val b = a.map(x => (x,x+1)) // add the first placeholder + 1 to the second place to get a pair of values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(Int, Int)] = Array((1,2), (2,3), (3,4), (4,5), (5,6), (6,7), (7,8), (8,9), (9,10), (10,11))\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataFrame: org.apache.spark.sql.DataFrame = [first: int, second: int]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataFrame = b.toDF(\"first\",\"second\")  // create a DataFrame w/ 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+\n",
      "|first|second|\n",
      "+-----+------+\n",
      "|    1|     2|\n",
      "|    2|     3|\n",
      "|    3|     4|\n",
      "|    4|     5|\n",
      "|    5|     6|\n",
      "|    6|     7|\n",
      "|    7|     8|\n",
      "|    8|     9|\n",
      "|    9|    10|\n",
      "|   10|    11|\n",
      "+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrame.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame from a List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cartoonCharacters: List[(String, Int)] = List((Bugs,1), (Elmer,5), (Daffy,3), (Sylvester,6), (Tweety,7), (Sam,9), (Porky,8), (Fog Horn,10), (Coyote,2), (Road Runner,4))\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cartoonCharacters = List((\"Bugs\", 1),(\"Elmer\", 5),(\"Daffy\", 3),(\"Sylvester\", 6),(\"Tweety\",7),\n",
    "                             (\"Sam\",9),(\"Porky\",8),(\"Fog Horn\",10),(\"Coyote\",2),(\"Road Runner\",4))\n",
    "// this is the same for a Sequence as well\n",
    "//  val cartoonCharacters = Seq((\"Bugs\", 1),(\"Elmer\", 5),(\"Daffy\", 3),(\"Sylvester\", 6),(\"Tweety\",7),\n",
    "//                             (\"Sam\",9),(\"Porky\",8),(\"Fog Horn\",10),(\"Coyote\",2),(\"Road Runner\",4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "characterPopularity: org.apache.spark.sql.DataFrame = [Name: string, Popularity: int]\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val characterPopularity = cartoonCharacters.toDF(\"Name\", \"Popularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|       Name|Popularity|\n",
      "+-----------+----------+\n",
      "|       Bugs|         1|\n",
      "|      Elmer|         5|\n",
      "|      Daffy|         3|\n",
      "|  Sylvester|         6|\n",
      "|     Tweety|         7|\n",
      "|        Sam|         9|\n",
      "|      Porky|         8|\n",
      "|   Fog Horn|        10|\n",
      "|     Coyote|         2|\n",
      "|Road Runner|         4|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "characterPopularity.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "// run some queries\n",
    "characterPopularity.registerTempTable(\"cartoonCharacters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Name|Popularity|\n",
      "+----+----------+\n",
      "|Bugs|         1|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM cartoonCharacters WHERE Name = 'Bugs'\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|      10|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from cartoonCharacters\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferring Schema using StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.{SQLContext, Row}\n",
       "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SQLContext,Row}\n",
    "import org.apache.spark.sql.types.{IntegerType, StringType, StructField, StructType}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(Name,StringType,true), StructField(Popluarity Rank,IntegerType,true))\n"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = StructType(Array(StructField(\"Name\", StringType,true),\n",
    "                    StructField(\"Popluarity Rank\", IntegerType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cartoonCharStructured: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[155] at map at <console>:28\n"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cartoonCharStructured = sc.parallelize(Seq(\"Tom\", \"Jerry\", \"Spike\")).map(x => (x,2+x.length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res69: Array[(String, Int)] = Array((Tom,5), (Jerry,7), (Spike,7))\n"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cartoonCharStructured.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "charStructuredRows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[157] at map at <console>:29\n"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val charStructuredRows = cartoonCharStructured.map(x => Row(x._1, x._2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res70: Array[org.apache.spark.sql.Row] = Array([Tom,5], [Jerry,7], [Spike,7])\n"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "charStructuredRows.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "charDataFrame: org.apache.spark.sql.DataFrame = [Name: string, Popluarity Rank: int]\n"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val charDataFrame = spark.createDataFrame(charStructuredRows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------+\n",
      "| Name|Popluarity Rank|\n",
      "+-----+---------------+\n",
      "|  Tom|              5|\n",
      "|Jerry|              7|\n",
      "|Spike|              7|\n",
      "+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "charDataFrame.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Inferred schema in class see the spark-shell-sparksql-inferred-schema-in-class.txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "starwarsCharacters: org.apache.spark.sql.DataFrame = [name: string, height: int ... 5 more fields]\n"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val starwarsCharacters = (spark.read.format(\"com.databricks.spark.csv\")\n",
    "                          .option(\"header\",\"true\")\n",
    "                          .option(\"inferSchema\",\"true\")\n",
    "                          .option(\"delimiter\",\",\").load(\"/spark-files/StarWars.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+------+------+--------+---------+-------+-----------+\n",
      "|            name|height|weight|eyecolor|haircolor|   jedi|    species|\n",
      "+----------------+------+------+--------+---------+-------+-----------+\n",
      "| nakin Skywalker|   188|    84|    blue|    blond|   jedi|      human|\n",
      "|   Padme Amidala|   165|    45|   brown|    brown|no_jedi|      human|\n",
      "|  Luke Skywalker|   172|    77|    blue|    blond|   jedi|      human|\n",
      "|  Leia Skywalker|   150|    49|   brown|    brown|no_jedi|      human|\n",
      "|    Qui-Gon Jinn|   193|    89|    blue|    brown|   jedi|      human|\n",
      "|  Obi-Wan Kenobi|   182|    77|bluegray|   auburn|   jedi|      human|\n",
      "|        Han Solo|   180|    80|   brown|    brown|no_jedi|      human|\n",
      "| Sheev Palpatine|   173|    75|    blue|      red|no_jedi|      human|\n",
      "|           R2-D2|    96|    32|    null|     null|no_jedi|      droid|\n",
      "|           C-3PO|   167|    75|    null|     null|no_jedi|      droid|\n",
      "|            Yoda|    66|    17|   brown|    brown|   jedi|       yoda|\n",
      "|      Darth Maul|   175|    80|  yellow|     none|no_jedi|dathomirian|\n",
      "|           Dooku|   193|    86|   brown|    brown|   jedi|      human|\n",
      "|       Chewbacca|   228|   112|    blue|    brown|no_jedi|    wookiee|\n",
      "|           Jabba|   390|  null|  yellow|     none|no_jedi|       hutt|\n",
      "|Lando Calrissian|   178|    79|   brown|    blank|no_jedi|      human|\n",
      "|       Boba Fett|   183|    78|   brown|    black|no_jedi|      human|\n",
      "|      Jango Fett|   183|    79|   brown|    black|no_jedi|      human|\n",
      "+----------------+------+------+--------+---------+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "starwarsCharacters.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "starwarsCharacters.registerTempTable(\"StarWarsCharacters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+--------+---------+-------+-------+\n",
      "|     name|height|weight|eyecolor|haircolor|   jedi|species|\n",
      "+---------+------+------+--------+---------+-------+-------+\n",
      "|Chewbacca|   228|   112|    blue|    brown|no_jedi|wookiee|\n",
      "+---------+------+------+--------+---------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM StarWarsCharacters WHERE species = 'wookiee'\").show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>spark-shell --packages com.databricks:spark-xml_2.10:0.4.1\n",
    "Ivy Default Cache set to: /home/hadoopuser/.ivy2/cache\n",
    "The jars for the packages stored in: /home/hadoopuser/.ivy2/jars\n",
    ":: loading settings :: url = jar:file:/usr/local/spark/spark-2.4.4/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
    "com.databricks#spark-xml_2.10 added as a dependency\n",
    ":: resolving dependencies :: org.apache.spark#spark-submit-parent-997ea178-376d-4ea1-9db6-008d5230a16f;1.0\n",
    "\tconfs: [default]\n",
    "\tfound com.databricks#spark-xml_2.10;0.4.1 in central\n",
    ":: resolution report :: resolve 236ms :: artifacts dl 3ms\n",
    "\t:: modules in use:\n",
    "\tcom.databricks#spark-xml_2.10;0.4.1 from central in [default]\n",
    "\t---------------------------------------------------------------------\n",
    "\t|                  |            modules            ||   artifacts   |\n",
    "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
    "\t---------------------------------------------------------------------\n",
    "\t|      default     |   1   |   0   |   0   |   0   ||   1   |   0   |\n",
    "\t---------------------------------------------------------------------\n",
    ":: retrieving :: org.apache.spark#spark-submit-parent-997ea178-376d-4ea1-9db6-008d5230a16f\n",
    "\tconfs: [default]\n",
    "\t0 artifacts copied, 1 already retrieved (0kB/7ms)\n",
    "24/06/16 23:55:02 WARN util.Utils: Your hostname, hadoopuser-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
    "24/06/16 23:55:02 WARN util.Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
    "24/06/16 23:55:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Setting default log level to &quot;WARN&quot;.\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "24/06/16 23:55:11 WARN util.Utils: Service &apos;SparkUI&apos; could not bind on port 4040. Attempting port 4041.\n",
    "24/06/16 23:55:11 WARN util.Utils: Service &apos;SparkUI&apos; could not bind on port 4041. Attempting port 4042.\n",
    "24/06/16 23:55:11 WARN util.Utils: Service &apos;SparkUI&apos; could not bind on port 4042. Attempting port 4043.\n",
    "24/06/16 23:55:11 WARN util.Utils: Service &apos;SparkUI&apos; could not bind on port 4043. Attempting port 4044.\n",
    "Spark context Web UI available at http://10.0.2.15:4044\n",
    "Spark context available as &apos;sc&apos; (master = local[*], app id = local-1718607311354).\n",
    "Spark session available as &apos;spark&apos;.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  &apos;_/\n",
    "   /___/ .__/\\_,_/_/ /_/\\_\\   version 2.4.4\n",
    "      /_/\n",
    "         \n",
    "Using Scala version 2.11.12 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_261)\n",
    "Type in expressions to have them evaluated.\n",
    "Type :help for more information.\n",
    "\n",
    "scala&gt; val employeeDF = spark.read.format(&quot;com.databricks.spark.xml&quot;).option(&quot;inferSchema&quot;,&quot;true&quot;).option(&quot;rootTag&quot;,&quot;employees&quot;).option(&quot;rowTag&quot;,&quot;employee&quot;).load(&quot;/spark-files/employees.xml&quot;)\n",
    "employeeDF: org.apache.spark.sql.DataFrame = [address: struct&lt;city: string, country: string ... 1 more field&gt;, dept_no: bigint ... 3 more fields]\n",
    "\n",
    "scala&gt; employeeDF.show\n",
    "+--------------------+-------+--------+------+------+\n",
    "|             address|dept_no|emp_name|emp_no|salary|\n",
    "+--------------------+-------+--------+------+------+\n",
    "|[Paris, London, 2...|      2|     jon|    10| 15000|\n",
    "|[Texas, America, ...|      5|    Adom|    11| 25000|\n",
    "+--------------------+-------+--------+------+------+\n",
    "\n",
    "\n",
    "scala&gt; \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "usstates: org.apache.spark.sql.DataFrame = [census_division: string, census_region: string ... 2 more fields]\n"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val usstates = spark.read.json(\"/spark-files/us_states.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res81: Array[org.apache.spark.sql.Row] = Array([East South Central,South,Alabama,AL], [Pacific,West,Alaska,AK], [Mountain,West,Arizona,AZ], [West South Central,South,Arkansas,AR], [Pacific,West,California,CA], [Mountain,West,Colorado,CO], [New England,Northeast,Connecticut,CT], [South Atlantic,South,Delaware,DE], [South Atlantic,South,District Of Columbia,DC], [South Atlantic,South,Florida,FL], [South Atlantic,South,Georgia,GA], [Pacific,West,Hawaii,HI], [Mountain,West,Idaho,ID], [East North Central,Midwest,Illinois,IL], [East North Central,Midwest,Indiana,IN], [West North Central,Midwest,Iowa,IA], [West North Central,Midwest,Kansas,KS], [East South Central,South,Kentucky,KY], [West South Central,South,Louisiana,LA], [New England,Northeast,Maine,ME], [South Atlantic,South,Maryland,MD], ..."
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usstates.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- census_division: string (nullable = true)\n",
      " |-- census_region: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usstates.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "usstates.registerTempTable(\"usstates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+--------------------+-----+\n",
      "|   census_division|census_region|                name|state|\n",
      "+------------------+-------------+--------------------+-----+\n",
      "|East South Central|        South|             Alabama|   AL|\n",
      "|           Pacific|         West|              Alaska|   AK|\n",
      "|          Mountain|         West|             Arizona|   AZ|\n",
      "|West South Central|        South|            Arkansas|   AR|\n",
      "|           Pacific|         West|          California|   CA|\n",
      "|          Mountain|         West|            Colorado|   CO|\n",
      "|       New England|    Northeast|         Connecticut|   CT|\n",
      "|    South Atlantic|        South|            Delaware|   DE|\n",
      "|    South Atlantic|        South|District Of Columbia|   DC|\n",
      "|    South Atlantic|        South|             Florida|   FL|\n",
      "|    South Atlantic|        South|             Georgia|   GA|\n",
      "|           Pacific|         West|              Hawaii|   HI|\n",
      "|          Mountain|         West|               Idaho|   ID|\n",
      "|East North Central|      Midwest|            Illinois|   IL|\n",
      "|East North Central|      Midwest|             Indiana|   IN|\n",
      "|West North Central|      Midwest|                Iowa|   IA|\n",
      "|West North Central|      Midwest|              Kansas|   KS|\n",
      "|East South Central|        South|            Kentucky|   KY|\n",
      "|West South Central|        South|           Louisiana|   LA|\n",
      "|       New England|    Northeast|               Maine|   ME|\n",
      "|    South Atlantic|        South|            Maryland|   MD|\n",
      "|                  |    Northeast|       Massachusetts|   MA|\n",
      "|       New England|      Midwest|            Michigan|   MI|\n",
      "|West North Central|      Midwest|           Minnesota|   MN|\n",
      "|East South Central|        South|         Mississippi|   MS|\n",
      "|West North Central|      Midwest|            Missouri|   MO|\n",
      "|          Mountain|         West|             Montana|   MT|\n",
      "|West North Central|      Midwest|            Nebraska|   NE|\n",
      "|          Mountain|         West|              Nevada|   NV|\n",
      "|       New England|    Northeast|       New Hampshire|   NH|\n",
      "|   Middle Atlantic|    Northeast|          New Jersey|   NJ|\n",
      "|          Mountain|         West|          New Mexico|   NM|\n",
      "|   Middle Atlantic|    Northeast|            New York|   NY|\n",
      "|    South Atlantic|        South|      North Carolina|   NC|\n",
      "|West North Central|      Midwest|        North Dakota|   ND|\n",
      "|East North Central|      Midwest|                Ohio|   OH|\n",
      "|West South Central|        South|            Oklahoma|   OK|\n",
      "|           Pacific|         West|              Oregon|   OR|\n",
      "|   Middle Atlantic|    Northeast|        Pennsylvania|   PA|\n",
      "|       New England|    Northeast|        Rhode Island|   RI|\n",
      "|    South Atlantic|        South|      South Carolina|   SC|\n",
      "|West North Central|      Midwest|        South Dakota|   SD|\n",
      "|East South Central|        South|           Tennessee|   TN|\n",
      "|West South Central|        South|               Texas|   TX|\n",
      "|          Mountain|         West|                Utah|   UT|\n",
      "|       New England|    Northeast|             Vermont|   VT|\n",
      "|    South Atlantic|        South|            Virginia|   VA|\n",
      "|           Pacific|         West|          Washington|   WA|\n",
      "|    South Atlantic|        South|       West Virginia|   WV|\n",
      "|East North Central|      Midwest|           Wisconsin|   WI|\n",
      "|          Mountain|         West|             Wyoming|   WY|\n",
      "+------------------+-------------+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM usstates\").show(51)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
